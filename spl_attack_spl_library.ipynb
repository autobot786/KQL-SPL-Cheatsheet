{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SPL command reference + attack detection library\n\nThis notebook gives you:\n1) **A complete, up-to-date list of Splunk SPL (SPL1) search commands** (with short descriptions)\n2) **A repeatable way to pull \u201cattack type\u201d detections (SPL searches) and organize them by MITRE ATT&CK** using Splunk Security Content.\n\n\u26a0\ufe0f **Defensive use only.** The detection queries included/collected here are intended for threat hunting and alerting.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 1 \u2014 SPL (SPL1) search command list\n\nThe next cells parse Splunk's *Command quick reference* into a table.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n        import re\n        import pandas as pd\n\n        RAW_COMMAND_TABLE = r\"\"\"` \u3010337\u2020abstract\u3011 ` Produces a summary of each search result. `highlight`\n` \u3010338\u2020accum\u3011 ` Keeps a running total of the specified numeric field. `autoregress, delta, trendline, streamstats`\n` \u3010339\u2020addcoltotals\u3011 ` Computes an event that contains sum of all numeric fields for previous events. `addtotals, stats`\n` \u3010340\u2020addinfo\u3011 ` Add fields that contain common information about the current search. `search`\n` \u3010341\u2020addtotals\u3011 ` Computes the sum of all numeric fields for each result. `addcoltotals`, `stats`\n` \u3010342\u2020analyzefields\u3011 ` Analyze numerical fields for their ability to predict another discrete field. `anomalousvalue`\n` \u3010343\u2020anomalies\u3011 ` Computes an \"unexpectedness\" score for an event. `anomalousvalue`, `cluster`, `kmeans`, `outlier`\n` \u3010344\u2020anomalousvalue\u3011 ` Finds and summarizes irregular, or uncommon, search results. `analyzefields`, `anomalies`, `cluster`, `kmeans`, `outlier`\n` \u3010345\u2020anomalydetection\u3011 ` Identifies anomalous events by computing a probability for each event and then detecting unusually small probabilities. `analyzefields`, `anomalies`, `anomalousvalue`, `cluster`, `kmeans`, `outlier`\n` \u3010346\u2020append\u3011 ` Appends subsearch results to current results. `appendcols`, `appendcsv`, `appendlookup`, `join`, `set`\n` \u3010347\u2020appendcols\u3011 ` Appends the fields of the subsearch results to current results, first results to first result, second to second, etc. `append`, `appendcsv`, `join`, `set`\n` \u3010348\u2020appendpipe\u3011 ` Appends the result of the subpipeline applied to the current result set to results. `append`, `appendcols`, `join`, `set`\n` \u3010349\u2020arules\u3011 ` Finds association rules between field values. `associate`, `correlate`\n` \u3010350\u2020associate\u3011 ` Identifies correlations between fields. `correlate`, `contingency`\n` \u3010351\u2020autoregress\u3011 ` Sets up data for calculating the moving average. `accum`, `autoregress`, `delta`, `trendline`, `streamstats`\n`\u3010352\u2020bin\u3011` (bucket) Puts continuous numerical values into discrete sets. `chart`, `timechart`\n` \u3010353\u2020bucketdir\u3011 ` Replaces a field value with higher-level grouping, such as replacing filenames with directories. `cluster`, `dedup`\n` \u3010354\u2020chart\u3011 ` Returns results in a tabular output for charting. See also, \u3010335\u2020Statistical and charting functions\u3011. `bin`,`sichart`, `timechart`\n` \u3010355\u2020cluster\u3011 ` Clusters similar events together. `anomalies`, `anomalousvalue`, `cluster`, `kmeans`, `outlier`\n` \u3010356\u2020cofilter\u3011 ` Finds how many times field1 and field2 values occurred together. `associate`, `correlate`\n` \u3010357\u2020collect\u3011 ` Puts search results into a summary index. `overlap`\n` \u3010358\u2020concurrency\u3011 ` Uses a duration field to find the number of \"concurrent\" events for each event. `timechart`\n` \u3010359\u2020contingency\u3011 ` Builds a contingency table for two fields. `associate`, `correlate`\n` \u3010360\u2020convert\u3011 ` Converts field values into numerical values. `eval`\n` \u3010361\u2020correlate\u3011 ` Calculates the correlation between different fields. `associate`, `contingency`\n` \u3010362\u2020datamodel\u3011 ` Examine data model or data model dataset and search a data model dataset. `pivot`\n` \u3010363\u2020dbinspect\u3011 ` Returns information about the specified index.\n` \u3010364\u2020dedup\u3011 ` Removes subsequent results that match a specified criteria. `uniq`\n` \u3010365\u2020delete\u3011 ` Delete specific events or search results.\n` \u3010366\u2020delta\u3011 ` Computes the difference in field value between nearby results. `accum`, `autoregress`, `trendline`, `streamstats`\n` \u3010367\u2020diff\u3011 ` Returns the difference between two search results.\n` \u3010368\u2020erex\u3011 ` Allows you to specify example or counter example values to automatically extract fields that have similar values. `extract`, `kvform`, `multikv`, `regex`, `rex`, `xmlkv`\n` \u3010369\u2020eval\u3011 ` Calculates an expression and puts the value into a field. See also, \u3010334\u2020Evaluation functions\u3011. `where`\n` \u3010370\u2020eventcount\u3011 ` Returns the number of events in an index. `dbinspect`\n` \u3010371\u2020eventstats\u3011 ` Adds summary statistics to all search results. `stats`\n`\u3010372\u2020extract\u3011` (kv) Extracts field-value pairs from search results. `kvform`, `multikv`, `xmlkv`, `rex`\n` \u3010373\u2020fieldformat\u3011 ` Expresses how to render a field at output time without changing the underlying value. `eval`, `where`\n` \u3010374\u2020fields\u3011 ` Keeps or removes fields from search results based on the field list criteria.\n` \u3010375\u2020fieldsummary\u3011 ` Generates summary information for all or a subset of the fields. `analyzefields`, `anomalies`, `anomalousvalue`, `stats`\n` \u3010376\u2020filldown\u3011 ` Replaces NULL values with the last non-NULL value. `fillnull`\n` \u3010377\u2020fillnull\u3011 ` Replaces null values with a specified value.\n` \u3010378\u2020findtypes\u3011 ` Generates a list of suggested event types. `typer`\n` \u3010379\u2020folderize\u3011 ` Creates a higher-level grouping, such as replacing filenames with directories.\n` \u3010380\u2020foreach\u3011 ` Run a templatized streaming subsearch for each field in a wildcarded field list. `eval`\n` \u3010381\u2020format\u3011 ` Takes the results of a subsearch and formats them into a single result.\n` \u3010382\u2020from\u3011 ` Retrieves data from a dataset, such as a data model dataset, a CSV lookup, a KV Store lookup, a saved search, or a table dataset.\n` \u3010383\u2020gauge\u3011 ` Transforms results into a format suitable for display by the Gauge chart types.\n` \u3010384\u2020gentimes\u3011 ` Generates time-range results.\n` \u3010385\u2020geom\u3011 ` Adds a field, named `geom`, to each event. This field contains geographic data structures for polygon geometry in JSON and is used for the choropleth map visualization. `geomfilter`\n` \u3010386\u2020geomfilter\u3011 ` Accepts two points that specify a bounding box for clipping a choropleth map. Points that fall outside of the bounding box are filtered out. `geom`\n` \u3010387\u2020geostats\u3011 ` Generate statistics which are clustered into geographical bins to be rendered on a world map. `stats, xyseries`\n` \u3010388\u2020head\u3011 ` Returns the first number `n` of specified results. `reverse, tail`\n` \u3010389\u2020highlight\u3011 ` Highlights the specified terms. `iconify`\n` \u3010390\u2020history\u3011 ` Returns a history of searches formatted as an events list or as a table. `search`\n` \u3010391\u2020iconify\u3011 ` Displays a unique icon for each different value in the list of fields that you specify. `highlight`\n` \u3010392\u2020inputcsv\u3011 ` Loads search results from the specified CSV file. `loadjob, outputcsv`\n` \u3010393\u2020inputlookup\u3011 ` Loads search results from a specified static lookup table. `inputcsv`, `join`, `lookup`, `outputlookup`\n` \u3010394\u2020iplocation\u3011 ` Extracts location information from IP addresses.\n` \u3010395\u2020join\u3011 ` Combine the results of a subsearch with the results of a main search. `appendcols, lookup, selfjoin`\n` \u3010396\u2020kmeans\u3011 ` Performs k-means clustering on selected fields. `anomalies, anomalousvalue, cluster, outlier`\n` \u3010397\u2020kvform\u3011 ` Extracts values from search results, using a form template. `extract, kvform, multikv, xmlkv, rex`\n` \u3010398\u2020loadjob\u3011 ` Loads events or results of a previously completed search job. `inputcsv`\n` \u3010399\u2020localize\u3011 ` Returns a list of the time ranges in which the search results were found. `map, transaction`\n` \u3010400\u2020localop\u3011 ` Run subsequent commands, that is all commands following this, locally and not on remote peers.\n` \u3010401\u2020lookup\u3011 ` Explicitly invokes field value lookups.\n` \u3010402\u2020makecontinuous\u3011 ` Makes a field that is supposed to be the x-axis continuous (invoked by chart/timechart) `chart, timechart`\n` \u3010403\u2020makemv\u3011 ` Change a specified field into a multivalued field during a search. `mvcombine, mvexpand, nomv`\n` \u3010404\u2020makeresults\u3011 ` Creates a specified number of empty search results.\n` \u3010405\u2020map\u3011 ` A looping operator, performs a search over each search result.\n` \u3010406\u2020mcollect\u3011 ` Converts search results into metric data and inserts the data into a metric index on the search head. `collect`, `meventcollect`\n` \u3010407\u2020metadata\u3011 ` Returns a list of source, sourcetypes, or hosts from a specified index or distributed search peer. `dbinspect`\n` \u3010408\u2020metasearch\u3011 ` Retrieves event metadata from indexes based on terms in the logical expression. `metadata`, `search`\n` \u3010409\u2020meventcollect\u3011 ` Converts search results into metric data and inserts the data into a metric index on the indexers. `collect`, `mcollect`\n` \u3010410\u2020mpreview\u3011 ` Returns a preview of the raw \u3010411\u2020metric data points\u2020docs.splunk.com\u3011 in a specified metric index that match a provided filter. `mcatalog`, `mstats`, `msearch`\n` \u3010410\u2020msearch\u3011 ` Alias for the `mpreview` command. `mcatalog`, `mstats`, `mpreview`\n` \u3010412\u2020mstats\u3011 ` Calculates statistics for the measurement, metric_name, and dimension fields in metric indexes. `stats`, `tstats`\n` \u3010413\u2020multikv\u3011 ` Extracts field-values from table-formatted events.\n` \u3010414\u2020multisearch\u3011 ` Run multiple \u3010415\u2020streaming searches\u2020docs.splunk.com\u3011 at the same time. `append, join`\n` \u3010416\u2020mvcombine\u3011 ` Combines events in search results that have a single differing field value into one result with a multivalue field of the differing field. `mvexpand, makemv, nomv`\n` \u3010417\u2020mvexpand\u3011 ` Expands the values of a multivalue field into separate events for each value of the multivalue field. `mvcombine, makemv, nomv`\n` \u3010418\u2020nomv\u3011 ` Changes a specified multivalued field into a single-value field at search time. `makemv, mvcombine, mvexpand`\n` \u3010419\u2020outlier\u3011 ` Removes outlying numerical values. `anomalies`, `anomalousvalue`, `cluster`, `kmeans`\n` \u3010420\u2020outputcsv\u3011 ` Outputs search results to a specified CSV file. `inputcsv`, `outputtext`\n` \u3010421\u2020outputlookup\u3011 ` Writes search results to the specified static lookup table. `inputlookup`, `lookup`, `outputcsv`\n` \u3010422\u2020outputtext\u3011 ` Outputs the raw text field (`_raw`) of results into the `_xml` field. `outputcsv`\n` \u3010423\u2020overlap\u3011 ` Finds events in a summary index that overlap in time or have missed events. `collect`\n` \u3010424\u2020pivot\u3011 ` Run pivot searches against a particular data model dataset. `datamodel`\n` \u3010425\u2020predict\u3011 ` Enables you to use time series algorithms to predict future values of fields. `x11`\n` \u3010426\u2020rangemap\u3011 ` Sets RANGE field to the name of the ranges that match.\n` \u3010427\u2020rare\u3011 ` Displays the least common values of a field. `sirare, stats, top`\n` \u3010428\u2020redistribute\u3011 ` Implements parallel reduce search processing to shorten the search runtime of high-cardinality dataset searches.\n` \u3010429\u2020regex\u3011 ` Removes results that do not match the specified regular expression. `rex`, `search`\n` \u3010430\u2020reltime\u3011 ` Converts the difference between 'now' and '_time' to a human-readable value and adds adds this value to the field, 'reltime', in your search results. `convert`\n` \u3010431\u2020rename\u3011 ` Renames a specified field; wildcards can be used to specify multiple fields.\n` \u3010432\u2020replace\u3011 ` Replaces values of specified fields with a specified new value.\n` \u3010433\u2020require\u3011 ` Causes a search to fail if the queries and commands that precede it in the search string return zero events or results.\n` \u3010434\u2020rest\u3011 ` Access a REST endpoint and display the returned entities as search results.\n` \u3010435\u2020return\u3011 ` Specify the values to return from a subsearch. `format, search`\n` \u3010436\u2020reverse\u3011 ` Reverses the order of the results. `head, sort, tail`\n` \u3010437\u2020rex\u3011 ` Specify a Perl regular expression named groups to extract fields while you search. `extract, kvform, multikv, xmlkv, regex`\n` \u3010438\u2020rtorder\u3011 ` Buffers events from real-time search to emit them in ascending time order when possible.\n` \u3010439\u2020savedsearch\u3011 ` Returns the search results of a saved search.\n`\u3010440\u2020script\u3011` (run) Runs an external Perl or Python script as part of your search.\n` \u3010441\u2020scrub\u3011 ` Anonymizes the search results.\n` \u3010442\u2020search\u3011 ` Searches indexes for matching events.\n` \u3010443\u2020searchtxn\u3011 ` Finds transaction events within specified search constraints. `transaction`\n` \u3010444\u2020selfjoin\u3011 ` Joins results with itself. `join`\n` \u3010445\u2020sendalert\u3011 ` invokes a custom alert action.\n` \u3010446\u2020sendemail\u3011 ` Emails search results to a specified email address.\n` \u3010447\u2020set\u3011 ` Performs set operations (union, diff, intersect) on subsearches. `append, appendcols, join, diff`\n` \u3010448\u2020setfields\u3011 ` Sets the field values for all results to a common value. `eval`, `fillnull`, `rename`\n` \u3010449\u2020sichart\u3011 ` Summary indexing version of the chart command. `chart, sitimechart, timechart`\n` \u3010450\u2020sirare\u3011 ` Summary indexing version of the rare command. `rare`\n` \u3010451\u2020sistats\u3011 ` Summary indexing version of the stats command. `stats`\n` \u3010452\u2020sitimechart\u3011 ` Summary indexing version of the timechart command. `chart, sichart, timechart`\n` \u3010453\u2020sitop\u3011 ` Summary indexing version of the top command. `top`\n` \u3010454\u2020sort\u3011 ` Sorts search results by the specified fields. `reverse`\n` \u3010455\u2020spath\u3011 ` Provides a straightforward means for extracting fields from structured data formats, XML and JSON. `xpath`\n` \u3010456\u2020stats\u3011 ` Provides statistics, grouped optionally by fields. See also, \u3010335\u2020Statistical and charting functions\u3011. `eventstats, top, rare`\n` \u3010457\u2020strcat\u3011 ` Concatenates string values.\n` \u3010458\u2020streamstats\u3011 ` Adds summary statistics to all search results in a streaming manner. `eventstats, stats`\n` \u3010459\u2020table\u3011 ` Creates a table using the specified fields. `fields`\n` \u3010460\u2020tags\u3011 ` Annotates specified fields in your search results with tags. `eval`\n` \u3010461\u2020tail\u3011 ` Returns the last number n of specified results. `head, reverse`\n` \u3010462\u2020timechart\u3011 ` Create a time series chart and corresponding table of statistics. See also, \u3010335\u2020Statistical and charting functions\u3011. `chart, bucket`\n` \u3010463\u2020timewrap\u3011 ` Displays, or wraps, the output of the \u3010462\u2020timechart command\u3011 so that every `timewrap-span` range of time is a different series. `timechart`\n` \u3010464\u2020tojson\u3011 ` Converts events into JSON objects.\n` \u3010465\u2020top\u3011 ` Displays the most common values of a field. `rare, stats`\n` \u3010466\u2020transaction\u3011 ` Groups search results into transactions.\n` \u3010467\u2020transpose\u3011 ` Reformats rows of search results as columns.\n` \u3010468\u2020trendline\u3011 ` Computes moving averages of fields. `timechart`\n` \u3010469\u2020tscollect\u3011 ` Writes results into tsidx file(s) for later use by the tstats command. `collect, stats, tstats`\n` \u3010470\u2020tstats\u3011 ` Calculates statistics over tsidx files created with the tscollect command. `stats, tscollect`\n` \u3010471\u2020typeahead\u3011 ` Returns typeahead information on a specified prefix.\n` \u3010472\u2020typelearner\u3011 ` Deprecated. Use `\u3010378\u2020findtypes\u3011` instead. Generates suggested eventtypes. `typer`\n` \u3010473\u2020typer\u3011 ` Calculates the eventtypes for the search results. `findtypes`\n` \u3010474\u2020union\u3011 ` Merges the results from two or more datasets into one dataset.\n` \u3010475\u2020uniq\u3011 ` Removes any search that is an exact duplicate with a previous result. `dedup`\n` \u3010476\u2020untable\u3011 ` Converts results from a tabular format to a format similar to `stats` output. Inverse of `xyseries` and `maketable`.\n` \u3010477\u2020walklex\u3011 ` Generates a list of terms or indexed fields from each bucket of event indexes. `metadata`, `tstats`\n` \u3010478\u2020where\u3011 ` Performs arbitrary filtering on your data. See also, \u3010334\u2020Evaluations functions\u3011. `eval`\n` \u3010479\u2020x11\u3011 ` Enables you to determine the trend in your data by removing the seasonal pattern. `predict`\n` \u3010480\u2020xmlkv\u3011 ` Extracts XML key-value pairs. `extract, kvform, multikv, rex`\n` \u3010481\u2020xmlunescape\u3011 ` Unescapes XML.\n` \u3010482\u2020xpath\u3011 ` Redefines the XML path.\n` \u3010483\u2020xyseries\u3011 ` Converts results into a format suitable for graphing.\"\"\"\n\n        def parse_spl_command_table(raw: str) -> pd.DataFrame:\n            rows = []\n            for line in raw.splitlines():\n                line = line.strip()\n                if not line:\n                    continue\n                m = re.match(r\"^`\\s*\u3010\\d+\u2020(?P<cmd>[^\u3011]+)\u3011\\s*`\\s*(?P<rest>.*)$\", line)\n                if not m:\n                    # Some lines may not match; ignore.\n                    continue\n                cmd = m.group(\"cmd\").strip()\n                rest = m.group(\"rest\").strip()\n\n                # Optional alias like \"(bucket)\" or \"(kv)\" or \"(run)\" immediately after the command.\n                alias = None\n                m_alias = re.match(r\"^\\(([^)]+)\\)\\s*(.*)$\", rest)\n                if m_alias:\n                    alias = m_alias.group(1).strip()\n                    rest = m_alias.group(2).strip()\n\n                # Extract backticked segments; later we'll map them to known command names.\n                bt_segments = re.findall(r\"`([^`]+)`\", rest)\n\n                # Remove all backticked segments from description (including code snippets like `n`).\n                desc = re.sub(r\"`[^`]+`\", \"\", rest)\n                desc = re.sub(r\"\\s+\", \" \", desc).strip()\n\n                rows.append({\n                    \"command\": cmd,\n                    \"alias\": alias,\n                    \"description\": desc,\n                    \"backtick_segments\": bt_segments\n                })\n\n            df = pd.DataFrame(rows).drop_duplicates(subset=[\"command\"]).reset_index(drop=True)\n\n            # Build a set of command names so we can filter related commands accurately.\n            cmd_set = set(df[\"command\"].tolist())\n\n            def related_from_segments(segments):\n                rel = []\n                for seg in segments or []:\n                    # split on comma, strip whitespace\n                    for token in [t.strip() for t in seg.split(\",\")]:\n                        if token in cmd_set and token != \"\":\n                            rel.append(token)\n                # unique, preserve order\n                seen = set()\n                rel_unique = []\n                for r in rel:\n                    if r not in seen:\n                        seen.add(r)\n                        rel_unique.append(r)\n                return rel_unique\n\n            df[\"related_commands\"] = df[\"backtick_segments\"].apply(related_from_segments)\n            df.drop(columns=[\"backtick_segments\"], inplace=True)\n\n            # Add internal commands (not in the public SPL command list table)\n            internal = ['collapse', 'dump', 'findkeywords', 'makejson', 'mcatalog', 'noop', 'prjob', 'redistribute', 'runshellscript']\n            for c in internal:\n                if c not in cmd_set:\n                    df = pd.concat([df, pd.DataFrame([{\"command\": c, \"alias\": \"internal\", \"description\": \"Internal command (may be unsupported/undocumented).\", \"related_commands\": []}])], ignore_index=True)\n\n            df = df.sort_values(by=[\"command\"]).reset_index(drop=True)\n            return df\n\n        spl_commands_df = parse_spl_command_table(RAW_COMMAND_TABLE)\n        spl_commands_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Quick checks\nprint(\"Total commands:\", len(spl_commands_df))\nprint(\"Example rows:\")\ndisplay(spl_commands_df.head(10))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 2 \u2014 \u201cAttack type\u201d SPL detections (MITRE ATT&CK)\n\nSplunk publishes a large set of detections (SPL searches) in **Splunk Security Content**.\nThe next section shows two ways to pull *all available* detections and organize them by ATT&CK:\n\n### Option A (API)\nUse the Splunk Research content API (if reachable from your environment).\n\n### Option B (Git)\nClone the `splunk/security_content` repo and parse the YAML detections.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# --- Option A: Pull detections via Splunk Research content API (preferred when available) ---\n#\n# This API has historically exposed endpoints like:\n#   https://content.splunkresearch.com/detections\n#   https://content.splunkresearch.com/stories\n#\n# If your environment blocks outbound internet, this won't work.\n\nimport json\nimport requests\nimport pandas as pd\n\nBASE = \"https://content.splunkresearch.com\"\n\ndef try_fetch_json(url: str, timeout=30):\n    r = requests.get(url, timeout=timeout)\n    r.raise_for_status()\n    return r.json()\n\ndetections_df_api = None\n\ntry:\n    root = try_fetch_json(BASE)\n    print(\"API root keys:\", list(root.keys())[:20])\n    # Try common endpoints\n    detections = try_fetch_json(f\"{BASE}/detections\")\n    # Expected: list/dict JSON; normalize\n    detections_df_api = pd.json_normalize(detections)\n    print(\"Fetched detections via API:\", len(detections_df_api))\n    display(detections_df_api.head(5))\nexcept Exception as e:\n    print(\"API method failed:\", e)\n    print(\"Try Option B (git clone + YAML parsing) instead.\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# --- Option B: Clone Splunk Security Content and parse YAML detections ---\n#\n# This pulls the latest content from GitHub.\n# If you don't have 'git', you can download the repo ZIP and point REPO_DIR to the extracted folder.\n\nimport os\nimport subprocess\nfrom pathlib import Path\n\nREPO_URL = \"https://github.com/splunk/security_content.git\"\nREPO_DIR = Path(\"security_content\")\n\nif not REPO_DIR.exists():\n    try:\n        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", \"develop\", REPO_URL, str(REPO_DIR)], check=True)\n    except Exception as e:\n        print(\"git clone failed:\", e)\n        print(\"You can manually download the repo ZIP and extract to ./security_content\")\nelse:\n    print(\"Repo already present:\", REPO_DIR.resolve())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Parse YAML detections into a DataFrame\n\nfrom pathlib import Path\n\ntry:\n    import yaml\nexcept ImportError:\n    import sys\n    !{sys.executable} -m pip install pyyaml\n    import yaml\n\nimport pandas as pd\n\ndef safe_list(x):\n    if x is None:\n        return []\n    if isinstance(x, list):\n        return x\n    return [x]\n\ndef flatten_tags(tags):\n    # tags is typically a dict; keep it as dict but also extract common MITRE fields if present\n    if not isinstance(tags, dict):\n        tags = {}\n    out = {\"tags\": tags}\n    for key in [\"mitre_attack_id\", \"mitre_attack_tactic\", \"mitre_attack_technique\", \"analytic_story\", \"security_domain\", \"kill_chain_phases\"]:\n        out[key] = tags.get(key)\n    return out\n\ndef load_detection_yaml(path: Path) -> dict:\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n\ndef build_detections_df(repo_dir: Path) -> pd.DataFrame:\n    det_root = repo_dir / \"detections\"\n    paths = list(det_root.rglob(\"*.yml\")) + list(det_root.rglob(\"*.yaml\"))\n    rows = []\n    for p in paths:\n        try:\n            d = load_detection_yaml(p) or {}\n            tags = d.get(\"tags\", {})\n            tag_flat = flatten_tags(tags)\n\n            rows.append({\n                \"file\": str(p.relative_to(repo_dir)),\n                \"name\": d.get(\"name\"),\n                \"id\": d.get(\"id\"),\n                \"version\": d.get(\"version\"),\n                \"status\": d.get(\"status\"),\n                \"type\": d.get(\"type\"),\n                \"description\": d.get(\"description\"),\n                \"search\": d.get(\"search\"),\n                \"data_source\": d.get(\"data_source\"),\n                \"how_to_implement\": d.get(\"how_to_implement\"),\n                \"known_false_positives\": d.get(\"known_false_positives\"),\n                \"references\": d.get(\"references\"),\n                **{k: tag_flat.get(k) for k in tag_flat if k != \"tags\"},\n                \"tags\": tag_flat.get(\"tags\"),\n            })\n        except Exception:\n            continue\n\n    df = pd.DataFrame(rows)\n    # Normalize some common tag fields to lists\n    for col in [\"mitre_attack_id\", \"mitre_attack_tactic\", \"analytic_story\", \"kill_chain_phases\"]:\n        if col in df.columns:\n            df[col] = df[col].apply(safe_list)\n    return df\n\ndetections_df = None\nif REPO_DIR.exists():\n    detections_df = build_detections_df(REPO_DIR)\n    print(\"Loaded detections:\", len(detections_df))\n    display(detections_df.head(5))\nelse:\n    print(\"Repo directory not found; please clone or download it first.\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Build a MITRE ATT&CK \u201cattack type\u201d index (tactic -> detections, technique ID -> detections)\n\nimport pandas as pd\n\nif detections_df is not None and len(detections_df):\n    # Explode tactics and technique IDs for easier grouping\n    tactics = detections_df.explode(\"mitre_attack_tactic\")\n    techniques = detections_df.explode(\"mitre_attack_id\")\n\n    tactic_counts = tactics.groupby(\"mitre_attack_tactic\")[\"name\"].count().sort_values(ascending=False)\n    technique_counts = techniques.groupby(\"mitre_attack_id\")[\"name\"].count().sort_values(ascending=False)\n\n    print(\"Detections by MITRE tactic (top 15):\")\n    display(tactic_counts.head(15))\n\n    print(\"Detections by MITRE technique ID (top 15):\")\n    display(technique_counts.head(15))\nelse:\n    print(\"No detections loaded yet.\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Helper: fetch all searches for a given tactic or technique and export to files\n\nfrom pathlib import Path\n\ndef export_spl_for_tactic(df: pd.DataFrame, tactic: str, out_dir: Path = Path(\"exported_spl\")):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    tactic_dir = out_dir / f\"tactic={tactic}\".replace(\" \", \"_\")\n    tactic_dir.mkdir(parents=True, exist_ok=True)\n\n    subset = df[df[\"mitre_attack_tactic\"].apply(lambda xs: tactic in (xs or []))].copy()\n    print(f\"Detections for tactic '{tactic}':\", len(subset))\n\n    for _, row in subset.iterrows():\n        name = (row.get(\"name\") or \"unnamed\").strip().replace(\"/\", \"_\")\n        file_path = tactic_dir / f\"{name}.spl\"\n        search = row.get(\"search\") or \"\"\n        file_path.write_text(search, encoding=\"utf-8\")\n\n    return subset\n\ndef export_spl_for_technique(df: pd.DataFrame, technique_id: str, out_dir: Path = Path(\"exported_spl\")):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    tech_dir = out_dir / f\"technique={technique_id}\".replace(\" \", \"_\")\n    tech_dir.mkdir(parents=True, exist_ok=True)\n\n    subset = df[df[\"mitre_attack_id\"].apply(lambda xs: technique_id in (xs or []))].copy()\n    print(f\"Detections for technique '{technique_id}':\", len(subset))\n\n    for _, row in subset.iterrows():\n        name = (row.get(\"name\") or \"unnamed\").strip().replace(\"/\", \"_\")\n        file_path = tech_dir / f\"{name}.spl\"\n        search = row.get(\"search\") or \"\"\n        file_path.write_text(search, encoding=\"utf-8\")\n\n    return subset\n\n# Example usage (uncomment and set a tactic you see above):\n# subset = export_spl_for_tactic(detections_df, \"discovery\")\n# subset = export_spl_for_technique(detections_df, \"T1033\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notes for adapting detections to your environment\n\n- Most searches assume certain **data sources** (Windows Security logs, Sysmon, EDR, DNS, proxy, cloud audit logs, etc.).\n- Adjust `index=...`, `sourcetype=...`, and field names to match your environment.\n- Prefer `tstats` / data models when using Splunk ES + accelerated data models for performance.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Appendix \u2014 Internal commands\n\nInternal commands exist but may be unsupported/undocumented. They are listed here for completeness.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\ninternal_commands = ['collapse', 'dump', 'findkeywords', 'makejson', 'mcatalog', 'noop', 'prjob', 'redistribute', 'runshellscript']\ninternal_commands\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "created": "2026-02-10T00:36:36.833519+00:00"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}